{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea7629d",
   "metadata": {},
   "source": [
    "## Exercise 1 Hello World\n",
    "\n",
    "1. Write an MPI program displaying the number of processes used for the execution and the rank of each process.\n",
    "2. Test the programs obtained with different numbers of threads for the parallel program.\n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "Hello from the rank 2 process\n",
    "Hello from the rank 0 process\n",
    "Hello from the rank 3 process\n",
    "Hello from the rank 1 process\n",
    "Parallel execution of hello_world with 4 process\n",
    "```\n",
    "*Note that the output order maybe different*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa95711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello.py\n"
     ]
    }
   ],
   "source": [
    " %%file hello.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "print(\"Hello from the rank {} process\".format(RANK))\n",
    "if RANK==nbOfproc-1:\n",
    "    print(\"Parallel execution of hello_world with {} process\".format(nbOfproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56fd20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the rank 3 process\r\n",
      "Parallel execution of hello_world with 4 process\r\n",
      "Hello from the rank 0 process\r\n",
      "Hello from the rank 1 process\r\n",
      "Hello from the rank 2 process\r\n"
     ]
    }
   ],
   "source": [
    "# enter command for compile and run the program\n",
    "! mpirun -n 4 python hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f24cd",
   "metadata": {},
   "source": [
    "## Exercise 2 Sharing Data \n",
    "\n",
    "A common need is for one process to get data from the user, either by reading from the terminal or command line arguments, and then to distribute this information to all other processors.\n",
    "\n",
    "Write a program that reads an integer value from the terminal and distributes the value to all of the MPI processes. Each process should print out its rank and the value it received. Values should be read until a negative integer is given as input.\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Get_rank` `Bcast` \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "10\n",
    "Process 0 got 10\n",
    "Process 1 got 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589033f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharing.py\n"
     ]
    }
   ],
   "source": [
    "%%file sharing.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "if RANK==0:\n",
    "    sendb=10  \n",
    "else:\n",
    "    sendb=None\n",
    "    \n",
    "recvb= COMM.bcast(sendb , root=0)\n",
    "if RANK==0:\n",
    "    print(recvb)\n",
    "print(\"Process {RANK} got {data}\".format(RANK=RANK, data=recvb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bafba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharing.py\n"
     ]
    }
   ],
   "source": [
    "%%file sharing.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "sendb=1\n",
    "recvb = 1\n",
    "while recvb > 0:\n",
    "    if RANK==0:\n",
    "        sendb=int(input())\n",
    "    recvb= COMM.bcast(sendb , root=0)\n",
    "    print(\"Process {RANK} got {data}\".format(RANK=RANK, data=recvb))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b061bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -n 2 python sharing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b683d",
   "metadata": {},
   "source": [
    "## Exercise 3 Sending in a ring (broadcast by ring)\n",
    "\n",
    "Write a program that takes data from process zero and sends it to all of the other processes by sending it in a ring. That is, process i should receive the data and send it to process i+1, until the last process is reached.\n",
    "Assume that the data consists of a single integer. Process zero reads the data from the user.\n",
    "![](../data/ring.gif)\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Send` `Recv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a981293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sending.py\n"
     ]
    }
   ],
   "source": [
    " %%file sending.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "tag=99\n",
    "i=0\n",
    "while i < nbOfproc-1:\n",
    "    if RANK==i:\n",
    "        sendb = 1000\n",
    "        COMM.send ( sendb , dest=i+1, tag=tag )\n",
    "    if RANK ==i+1:\n",
    "        recvb = COMM.recv ( source=i , tag=tag )\n",
    "        print (\"Process {RANK} receive {recvb} from {RANKO}\".format(RANK=RANK, recvb=recvb, RANKO=RANK-1))\n",
    "    i=i+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54725d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1 receive 1000 from 0\r\n",
      "Process 2 receive 1000 from 1\r\n",
      "Process 3 receive 1000 from 2\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -n 4 python sending.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a06f2d",
   "metadata": {},
   "source": [
    "## Exercise 4 Matrix vector product\n",
    "\n",
    "1. Use the `MatrixVectorMult.py` file to implement the MPI version of matrix vector multiplication.\n",
    "2. Process 0 compares the result with the `dot` product.\n",
    "3. Plot the scalability of your implementation. \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "CPU time of parallel multiplication using 2 processes is  174.923446\n",
    "The error comparing to the dot product is : 1.4210854715202004e-14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb54b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3104b2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MatrixVectorMult_V0.py\n"
     ]
    }
   ],
   "source": [
    " %%file MatrixVectorMult_V0.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "''' This program compute parallel csc matrix vector multiplication using mpi '''\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "\n",
    "def matrixVectorMult(A, b, x):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += a[j] * b[j]\n",
    "\n",
    "    return 0\n",
    "\n",
    "########################initialize matrix A and vector b ######################\n",
    "#matrix sizes\n",
    "SIZE = 1000\n",
    "Local_size = SIZE//nbOfproc\n",
    "\n",
    "# counts = block of each proc\n",
    "#counts = \n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray()\n",
    "    b = rand(SIZE)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "\n",
    "\n",
    "#########Send b to all procs and scatter A (each proc has its own local matrix#####\n",
    "\n",
    "LocalMatrix = np.zeros((Local_size, SIZE))\n",
    "COMM.Scatter(A, LocalMatrix, root=0)\n",
    "\n",
    "# Scatter the matrix A\n",
    "b=COMM.bcast(b,root=0)\n",
    "\n",
    "\n",
    "\n",
    "#####################Compute A*b locally#######################################\n",
    "LocalX = np.zeros((Local_size, SIZE))\n",
    "\n",
    "\n",
    "start = MPI.Wtime()\n",
    "matrixVectorMult(LocalMatrix, b, LocalX)\n",
    "print(LocalX)\n",
    "stop = MPI.Wtime()\n",
    "if RANK == 0:\n",
    "    print(\"CPU time of parallel multiplication is \", (stop - start)*1000)\n",
    "\n",
    "##################Gather te results ###########################################\n",
    "sendcouns = len(LocalX)\n",
    "sendcounts = np.array(COMM.gather(sendcouns,root=0))\n",
    "if RANK == 0: \n",
    "     X = np.empty(sum(sendcounts),dtype=int)\n",
    "else :\n",
    "     X = None\n",
    "\n",
    "# Gather the result into X\n",
    "COMM.Gatherv(LocalX, X=(X, sendcounts,[SIZE], MPI.DOUBLE), root=0)\n",
    "\n",
    "\n",
    "##################Print the results ###########################################\n",
    "\n",
    "if RANK == 0 :\n",
    "    X_ = A.dot(b)\n",
    "    print(\"The result of A*b using dot is :\", np.max(X_ - X))\n",
    "    # print(\"The result of A*b using parallel version is :\", X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f334a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01917762 0.01917762 0.01917762 ... 0.01917762 0.01917762 0.01917762]\n",
      " [0.61192795 0.61192795 0.61192795 ... 0.61192795 0.61192795 0.61192795]\n",
      " [0.18080638 0.18080638 0.18080638 ... 0.18080638 0.18080638 0.18080638]\n",
      " ...\n",
      " [0.11831044 0.11831044 0.11831044 ... 0.11831044 0.11831044 0.11831044]\n",
      " [0.62204149 0.62204149 0.62204149 ... 0.62204149 0.62204149 0.62204149]\n",
      " [0.13863926 0.13863926 0.13863926 ... 0.13863926 0.13863926 0.13863926]]\n",
      "Traceback (most recent call last):\n",
      "  File \"MatrixVectorMult_V0.py\", line 78, in <module>\n",
      "    COMM.Gatherv(LocalX, X=(X, sendcounts,[SIZE], MPI.DOUBLE), root=0)\n",
      "  File \"mpi4py/MPI/Comm.pyx\", line 711, in mpi4py.MPI.Comm.Gatherv\n",
      "TypeError: Gatherv() takes at least 2 positional arguments (1 given)\n",
      "[[0.03241312 0.03241312 0.03241312 ... 0.03241312 0.03241312 0.03241312]\n",
      " [0.09008819 0.09008819 0.09008819 ... 0.09008819 0.09008819 0.09008819]\n",
      " [0.04012056 0.04012056 0.04012056 ... 0.04012056 0.04012056 0.04012056]\n",
      " ...\n",
      " [0.19685325 0.19685325 0.19685325 ... 0.19685325 0.19685325 0.19685325]\n",
      " [0.44540926 0.44540926 0.44540926 ... 0.44540926 0.44540926 0.44540926]\n",
      " [0.38450608 0.38450608 0.38450608 ... 0.38450608 0.38450608 0.38450608]]\n",
      "Traceback (most recent call last):\n",
      "  File \"MatrixVectorMult_V0.py\", line 78, in <module>\n",
      "    COMM.Gatherv(LocalX, X=(X, sendcounts,[SIZE], MPI.DOUBLE), root=0)\n",
      "  File \"mpi4py/MPI/Comm.pyx\", line 711, in mpi4py.MPI.Comm.Gatherv\n",
      "TypeError: Gatherv() takes at least 2 positional arguments (1 given)\n",
      "[[2.63781326e+01 2.63781326e+01 2.63781326e+01 ... 2.63781326e+01\n",
      "  2.63781326e+01 2.63781326e+01]\n",
      " [2.13071154e+01 2.13071154e+01 2.13071154e+01 ... 2.13071154e+01\n",
      "  2.13071154e+01 2.13071154e+01]\n",
      " [2.77823379e-01 2.77823379e-01 2.77823379e-01 ... 2.77823379e-01\n",
      "  2.77823379e-01 2.77823379e-01]\n",
      " ...\n",
      " [1.68095437e-02 1.68095437e-02 1.68095437e-02 ... 1.68095437e-02\n",
      "  1.68095437e-02 1.68095437e-02]\n",
      " [3.69985985e-01 3.69985985e-01 3.69985985e-01 ... 3.69985985e-01\n",
      "  3.69985985e-01 3.69985985e-01]\n",
      " [2.51027352e-02 2.51027352e-02 2.51027352e-02 ... 2.51027352e-02\n",
      "  2.51027352e-02 2.51027352e-02]]\n",
      "CPU time of parallel multiplication is  890.682976\n",
      "[[0.43623498 0.43623498 0.43623498 ... 0.43623498 0.43623498 0.43623498]\n",
      " [0.49562961 0.49562961 0.49562961 ... 0.49562961 0.49562961 0.49562961]\n",
      " [0.28891318 0.28891318 0.28891318 ... 0.28891318 0.28891318 0.28891318]\n",
      " ...\n",
      " [0.01368554 0.01368554 0.01368554 ... 0.01368554 0.01368554 0.01368554]\n",
      " [0.01647114 0.01647114 0.01647114 ... 0.01647114 0.01647114 0.01647114]\n",
      " [0.03654434 0.03654434 0.03654434 ... 0.03654434 0.03654434 0.03654434]]\n",
      "Traceback (most recent call last):\n",
      "  File \"MatrixVectorMult_V0.py\", line 78, in <module>\n",
      "    COMM.Gatherv(LocalX, X=(X, sendcounts,[SIZE], MPI.DOUBLE), root=0)\n",
      "  File \"mpi4py/MPI/Comm.pyx\", line 711, in mpi4py.MPI.Comm.Gatherv\n",
      "TypeError: Gatherv() takes at least 2 positional arguments (1 given)\n",
      "Traceback (most recent call last):\n",
      "  File \"MatrixVectorMult_V0.py\", line 78, in <module>\n",
      "    COMM.Gatherv(LocalX, X=(X, sendcounts,[SIZE], MPI.DOUBLE), root=0)\n",
      "  File \"mpi4py/MPI/Comm.pyx\", line 711, in mpi4py.MPI.Comm.Gatherv\n",
      "TypeError: Gatherv() takes at least 2 positional arguments (1 given)\n",
      "--------------------------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpirun detected that one or more processes exited with non-zero status, thus causing\n",
      "the job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[45193,1],0]\n",
      "  Exit code:    1\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! mpirun -n 4 python MatrixVectorMult_V0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc2847",
   "metadata": {},
   "source": [
    "## Exercise 5 Calculation of π (Monte Carlo)\n",
    "\n",
    "1. Use the `PiMonteCarlo.py` file to implement the calculation of PI using Monte Carlo.\n",
    "2. Process 0 prints the result.\n",
    "3. Plot the scalability of your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd89c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PiMonteCarlo_V0.py\n"
     ]
    }
   ],
   "source": [
    " %%file PiMonteCarlo_V0.py\n",
    "import random \n",
    "import timeit\n",
    "\n",
    "INTERVAL= 1000\n",
    "\n",
    "random.seed(42)  \n",
    "\n",
    "def compute_points():\n",
    "    \n",
    "    random.seed(42)  \n",
    "    \n",
    "    circle_points= 0\n",
    "\n",
    "    # Total Random numbers generated= possible x \n",
    "    # values* possible y values \n",
    "    for i in range(INTERVAL**2): \n",
    "      \n",
    "        # Randomly generated x and y values from a \n",
    "        # uniform distribution \n",
    "        # Rannge of x and y values is -1 to 1 \n",
    "                \n",
    "        rand_x= random.uniform(-1, 1) \n",
    "        rand_y= random.uniform(-1, 1) \n",
    "      \n",
    "        # Distance between (x, y) from the origin \n",
    "        origin_dist= rand_x**2 + rand_y**2\n",
    "      \n",
    "        # Checking if (x, y) lies inside the circle \n",
    "        if origin_dist<= 1: \n",
    "            circle_points+= 1\n",
    "      \n",
    "        # Estimating value of pi, \n",
    "        # pi= 4*(no. of points generated inside the  \n",
    "        # circle)/ (no. of points generated inside the square) \n",
    "    \n",
    "     \n",
    "    \n",
    "    return circle_points\n",
    "\n",
    "start = timeit.default_timer()\n",
    "circle_points = compute_points()\n",
    "end = timeit.default_timer()\n",
    "\n",
    "\n",
    "pi = 4* circle_points/ INTERVAL**2 \n",
    "print(\"Circle points number :\",circle_points)\n",
    "print(\"Final Estimation of Pi=\", pi, \"cpu time :\",end-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6e810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circle points number : 785061\r\n",
      "Final Estimation of Pi= 3.140244 cpu time : 0.6954496919997837\r\n",
      "Circle points number : 785061\r\n",
      "Final Estimation of Pi= 3.140244 cpu time : 0.6976209250005923\r\n",
      "Circle points number : 785061\r\n",
      "Final Estimation of Pi= 3.140244 cpu time : 0.7041604629994254\r\n",
      "Circle points number : 785061\r\n",
      "Final Estimation of Pi= 3.140244 cpu time : 0.7082776609995562\r\n"
     ]
    }
   ],
   "source": [
    "! mpirun -n 4 python PiMonteCarlo_V0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0dce87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scatter-array.py\n"
     ]
    }
   ],
   "source": [
    "%%file scatter-array.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "nprocs = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    sendbuf = np.arange(15.0)\n",
    "\n",
    "    # count: the size of each sub-task\n",
    "    ave, res = divmod(sendbuf.size, nprocs)\n",
    "    print(ave,res)\n",
    "    count = [ave + 1 if p < res else ave for p in range(nprocs)]\n",
    "    print(count)\n",
    "    count = np.array(count)\n",
    "    print(count)\n",
    "    # displacement: the starting index of each sub-task\n",
    "    displ = [sum(count[:p]) for p in range(nprocs)]\n",
    "    displ = np.array(displ)\n",
    "    print(displ)\n",
    "else:\n",
    "    sendbuf = None\n",
    "    # initialize count on worker processes\n",
    "    count = np.zeros(nprocs, dtype=np.int)\n",
    "    displ = None\n",
    "\n",
    "# broadcast count\n",
    "comm.Bcast(count, root=0)\n",
    "\n",
    "# initialize recvbuf on all processes\n",
    "recvbuf = np.zeros(count[rank])\n",
    "\n",
    "comm.Scatterv([sendbuf, count, displ, MPI.DOUBLE], recvbuf, root=0)\n",
    "\n",
    "print('After Scatterv, process {} has data:'.format(rank), recvbuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4c4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "[4, 4, 4, 3]\n",
      "[4 4 4 3]\n",
      "[ 0  4  8 12]\n",
      "After Scatterv, process 0 has data: [0. 1. 2. 3.]\n",
      "After Scatterv, process 1 has data: [4. 5. 6. 7.]\n",
      "scatter-array.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  count = np.zeros(nprocs, dtype=np.int)\n",
      "After Scatterv, process 3 has data: [12. 13. 14.]\n",
      "scatter-array.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  count = np.zeros(nprocs, dtype=np.int)\n",
      "After Scatterv, process 2 has data: [ 8.  9. 10. 11.]\n",
      "scatter-array.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  count = np.zeros(nprocs, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3 scatter-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e951e7e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recvbuf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sendbuf2 \u001b[38;5;241m=\u001b[39m \u001b[43mrecvbuf\u001b[49m\n\u001b[1;32m      2\u001b[0m recvbuf2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28msum\u001b[39m(count))\n\u001b[1;32m      3\u001b[0m comm\u001b[38;5;241m.\u001b[39mGatherv(sendbuf2, [recvbuf2, count, displ, MPI\u001b[38;5;241m.\u001b[39mDOUBLE], root\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recvbuf' is not defined"
     ]
    }
   ],
   "source": [
    "sendbuf2 = recvbuf\n",
    "recvbuf2 = np.zeros(sum(count))\n",
    "comm.Gatherv(sendbuf2, [recvbuf2, count, displ, MPI.DOUBLE], root=0)\n",
    "\n",
    "if comm.Get_rank() == 0:\n",
    "    print('After Gatherv, process 0 has data:', recvbuf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e35bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
